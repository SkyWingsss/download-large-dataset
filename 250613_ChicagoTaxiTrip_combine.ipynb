{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77140efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/skywings/Documents/RA/Professor_Zhao_RA/ä»»åŠ¡/chicago-taxi-data')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "root = Path.cwd().parent\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef2e3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 23 columns):\n",
      " #   Column                  Non-Null Count    Dtype  \n",
      "---  ------                  --------------    -----  \n",
      " 0   unique_key              1000000 non-null  object \n",
      " 1   taxi_id                 1000000 non-null  object \n",
      " 2   trip_start_timestamp    1000000 non-null  object \n",
      " 3   trip_end_timestamp      1000000 non-null  object \n",
      " 4   trip_seconds            999628 non-null   float64\n",
      " 5   trip_miles              1000000 non-null  float64\n",
      " 6   pickup_census_tract     23825 non-null    float64\n",
      " 7   dropoff_census_tract    35542 non-null    float64\n",
      " 8   pickup_community_area   30401 non-null    float64\n",
      " 9   dropoff_community_area  73067 non-null    float64\n",
      " 10  fare                    999904 non-null   float64\n",
      " 11  tips                    999904 non-null   float64\n",
      " 12  tolls                   996680 non-null   float64\n",
      " 13  extras                  999904 non-null   float64\n",
      " 14  trip_total              999904 non-null   float64\n",
      " 15  payment_type            1000000 non-null  object \n",
      " 16  company                 3224 non-null     object \n",
      " 17  pickup_latitude         30405 non-null    float64\n",
      " 18  pickup_longitude        30405 non-null    float64\n",
      " 19  pickup_location         30405 non-null    object \n",
      " 20  dropoff_latitude        73255 non-null    float64\n",
      " 21  dropoff_longitude       73255 non-null    float64\n",
      " 22  dropoff_location        73255 non-null    object \n",
      "dtypes: float64(15), object(8)\n",
      "memory usage: 175.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_key</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>trip_end_timestamp</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pickup_census_tract</th>\n",
       "      <th>dropoff_census_tract</th>\n",
       "      <th>pickup_community_area</th>\n",
       "      <th>dropoff_community_area</th>\n",
       "      <th>...</th>\n",
       "      <th>extras</th>\n",
       "      <th>trip_total</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>company</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_location</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18b092309dff1fa8298b21f8dfb9be19124363c4</td>\n",
       "      <td>193f49f2e6f3c31a0058bb26e322f818043e6d64ff80a9...</td>\n",
       "      <td>2014-06-07 23:15:00 UTC</td>\n",
       "      <td>2014-06-07 23:15:00 UTC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.703182e+10</td>\n",
       "      <td>1.703182e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b0601c2eaeccb6c7629f48858a007afea2e98e4</td>\n",
       "      <td>2cb7deb7674470467d31b1bba4657ab1c44c1feebf6274...</td>\n",
       "      <td>2014-04-16 15:30:00 UTC</td>\n",
       "      <td>2014-04-16 15:30:00 UTC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.986712</td>\n",
       "      <td>-87.663416</td>\n",
       "      <td>POINT (-87.6634164054 41.9867117999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0e1861cfb146b9e1b8b773b5be89f69cee92a0fc</td>\n",
       "      <td>e055c27835840cb1b08c8f68e20066307ab235a02fe7bd...</td>\n",
       "      <td>2014-04-25 14:15:00 UTC</td>\n",
       "      <td>2014-04-25 14:15:00 UTC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f8583c9d5d9fde21f8bcdd7b3ed0439c21a7fd88</td>\n",
       "      <td>e055c27835840cb1b08c8f68e20066307ab235a02fe7bd...</td>\n",
       "      <td>2014-04-25 14:15:00 UTC</td>\n",
       "      <td>2014-04-25 14:15:00 UTC</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5551bd37929928c32a1e703f871cedd1978709f0</td>\n",
       "      <td>e055c27835840cb1b08c8f68e20066307ab235a02fe7bd...</td>\n",
       "      <td>2014-04-25 16:30:00 UTC</td>\n",
       "      <td>2014-04-25 16:30:00 UTC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 unique_key  \\\n",
       "0  18b092309dff1fa8298b21f8dfb9be19124363c4   \n",
       "1  7b0601c2eaeccb6c7629f48858a007afea2e98e4   \n",
       "2  0e1861cfb146b9e1b8b773b5be89f69cee92a0fc   \n",
       "3  f8583c9d5d9fde21f8bcdd7b3ed0439c21a7fd88   \n",
       "4  5551bd37929928c32a1e703f871cedd1978709f0   \n",
       "\n",
       "                                             taxi_id     trip_start_timestamp  \\\n",
       "0  193f49f2e6f3c31a0058bb26e322f818043e6d64ff80a9...  2014-06-07 23:15:00 UTC   \n",
       "1  2cb7deb7674470467d31b1bba4657ab1c44c1feebf6274...  2014-04-16 15:30:00 UTC   \n",
       "2  e055c27835840cb1b08c8f68e20066307ab235a02fe7bd...  2014-04-25 14:15:00 UTC   \n",
       "3  e055c27835840cb1b08c8f68e20066307ab235a02fe7bd...  2014-04-25 14:15:00 UTC   \n",
       "4  e055c27835840cb1b08c8f68e20066307ab235a02fe7bd...  2014-04-25 16:30:00 UTC   \n",
       "\n",
       "        trip_end_timestamp  trip_seconds  trip_miles  pickup_census_tract  \\\n",
       "0  2014-06-07 23:15:00 UTC           0.0         0.0         1.703182e+10   \n",
       "1  2014-04-16 15:30:00 UTC           0.0         0.0                  NaN   \n",
       "2  2014-04-25 14:15:00 UTC           0.0         0.0                  NaN   \n",
       "3  2014-04-25 14:15:00 UTC          60.0         0.0                  NaN   \n",
       "4  2014-04-25 16:30:00 UTC           0.0         0.0                  NaN   \n",
       "\n",
       "   dropoff_census_tract  pickup_community_area  dropoff_community_area  ...  \\\n",
       "0          1.703182e+10                    NaN                     NaN  ...   \n",
       "1                   NaN                    NaN                    77.0  ...   \n",
       "2                   NaN                    NaN                     NaN  ...   \n",
       "3                   NaN                    NaN                     NaN  ...   \n",
       "4                   NaN                    NaN                     NaN  ...   \n",
       "\n",
       "   extras  trip_total  payment_type  company  pickup_latitude  \\\n",
       "0     NaN         NaN          Cash      NaN              NaN   \n",
       "1     NaN         NaN   Credit Card      NaN              NaN   \n",
       "2     NaN         NaN   Credit Card      NaN              NaN   \n",
       "3     NaN         NaN   Credit Card      NaN              NaN   \n",
       "4     NaN         NaN   Credit Card      NaN              NaN   \n",
       "\n",
       "  pickup_longitude pickup_location  dropoff_latitude  dropoff_longitude  \\\n",
       "0              NaN             NaN               NaN                NaN   \n",
       "1              NaN             NaN         41.986712         -87.663416   \n",
       "2              NaN             NaN               NaN                NaN   \n",
       "3              NaN             NaN               NaN                NaN   \n",
       "4              NaN             NaN               NaN                NaN   \n",
       "\n",
       "                       dropoff_location  \n",
       "0                                   NaN  \n",
       "1  POINT (-87.6634164054 41.9867117999)  \n",
       "2                                   NaN  \n",
       "3                                   NaN  \n",
       "4                                   NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sample = pd.read_csv(root / 'input' / 'chicago_taxi_trips_000000000003.csv')\n",
    "display(df_sample.head(), df_sample.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c5a735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ å‘ç° 212 ä¸ªCSVæ–‡ä»¶ï¼Œå°†ä»¥æ¯æ‰¹ 10 ä¸ªæ–‡ä»¶çš„è§„æ¨¡å¤„ç†ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å¤„ç†CSVæ‰¹æ¬¡å¹¶ç”ŸæˆParquet:   0%|          | 0/22 [00:00<?, ?it/s]/var/folders/gx/8b80zx6d2dd0slh0770_q04w0000gn/T/ipykernel_33918/1042761720.py:34: DeprecationWarning: the argument `dtypes` for `read_csv` is deprecated. It was renamed to `schema_overrides` in version 0.20.31.\n",
      "  df = pl.read_csv(\n",
      "å¤„ç†CSVæ‰¹æ¬¡å¹¶ç”ŸæˆParquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [04:01<00:00, 10.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… æ‰€æœ‰CSVæ‰¹æ¬¡å‡å·²å¤„ç†å¹¶ä¿å­˜ä¸ºä¸´æ—¶çš„ Parquet æ–‡ä»¶ã€‚\n",
      "â³ å¼€å§‹å°†æ¯ä¸ª Parquet åˆ†å—ç‹¬ç«‹è½¬æ¢ä¸º .dta æ–‡ä»¶...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è½¬æ¢Parquetåˆ†å—ä¸ºDTA: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [16:02<00:00, 43.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… æ‰€æœ‰åˆ†å—å¤„ç†å®Œæ¯•ï¼\n",
      "ğŸ“‚ è¯·åœ¨è¾“å‡ºç›®å½•æŸ¥çœ‹ç”Ÿæˆçš„ `..._part_N.dta` æ–‡ä»¶ã€‚\n",
      "ğŸ—‘ï¸ æ­£åœ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶...\n",
      "âœ¨ æ“ä½œå®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil  # ç”¨äºå¤„ç†æ–‡ä»¶å¤¹\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. æ ‡å‡†ç»“æ„å®šä¹‰ (ä¸æ‚¨åŸæ¥çš„ä¸€è‡´ï¼Œæ— éœ€æ”¹åŠ¨)\n",
    "# å®šä¹‰æ‰€æœ‰åˆ—çš„ç»Ÿä¸€æ•°æ®ç±»å‹å’Œé¡ºåº\n",
    "STANDARD_SCHEMA = {\n",
    "    \"unique_key\": pl.Utf8, \"taxi_id\": pl.Utf8, \"trip_start_timestamp\": pl.Utf8,\n",
    "    \"trip_end_timestamp\": pl.Utf8, \"trip_seconds\": pl.Float64, \"trip_miles\": pl.Float64,\n",
    "    \"pickup_census_tract\": pl.Float64, \"dropoff_census_tract\": pl.Float64,\n",
    "    \"pickup_community_area\": pl.Float64, \"dropoff_community_area\": pl.Float64,\n",
    "    \"fare\": pl.Float64, \"tips\": pl.Float64, \"tolls\": pl.Float64,\n",
    "    \"extras\": pl.Float64, \"trip_total\": pl.Float64, \"payment_type\": pl.Utf8,\n",
    "    \"company\": pl.Utf8, \"pickup_latitude\": pl.Float64, \"pickup_longitude\": pl.Float64,\n",
    "    \"pickup_location\": pl.Utf8, \"dropoff_latitude\": pl.Float64,\n",
    "    \"dropoff_longitude\": pl.Float64, \"dropoff_location\": pl.Utf8\n",
    "}\n",
    "STANDARD_COLS = list(STANDARD_SCHEMA.keys())\n",
    "\n",
    "# 2. å®‰å…¨è¯»å–CSVçš„å‡½æ•° (ä¸æ‚¨åŸæ¥çš„ä¸€è‡´ï¼Œæ— éœ€æ”¹åŠ¨)\n",
    "# è¿™ä¸ªå‡½æ•°èƒ½å¥å£®åœ°å¤„ç†åˆ—åä¸ä¸€è‡´ã€åˆ—ç¼ºå¤±ç­‰é—®é¢˜\n",
    "def robust_csv_reader(file_path):\n",
    "    \"\"\"\n",
    "    å®‰å…¨åœ°è¯»å–å•ä¸ªCSVæ–‡ä»¶ï¼Œå¤„ç†å¯èƒ½çš„é”™è¯¯ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„Polars DataFrameã€‚\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ä½¿ç”¨ Polars è¯»å– CSVï¼Œåˆå§‹å…¨éƒ¨ä½œä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œé¿å…æ¨æ–­é”™è¯¯\n",
    "        df = pl.read_csv(\n",
    "            file_path, infer_schema_length=0,\n",
    "            dtypes={col: pl.Utf8 for col in STANDARD_COLS},\n",
    "            null_values=[\"\", \"NA\", \"NULL\", \"N/A\", \"NaN\", \".\"],\n",
    "            ignore_errors=True\n",
    "        )\n",
    "        # æ¸…ç†åˆ—åï¼šè½¬ä¸ºå°å†™ï¼Œæ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿\n",
    "        df = df.rename(lambda col: col.strip().lower().replace(\" \", \"_\"))\n",
    "        # ç¡®ä¿æ‰€æœ‰æ ‡å‡†åˆ—éƒ½å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™ç”¨nullå¡«å……\n",
    "        for col in STANDARD_COLS:\n",
    "            if col not in df.columns:\n",
    "                df = df.with_columns(pl.lit(None, dtype=pl.Utf8).alias(col))\n",
    "        # é€‰æ‹©æ ‡å‡†åˆ—å¹¶æŒ‰ç…§æ ‡å‡†schemaè¿›è¡Œç±»å‹è½¬æ¢\n",
    "        return df.select(STANDARD_COLS).cast(STANDARD_SCHEMA, strict=False)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ æ–‡ä»¶ {os.path.basename(file_path)} è¯»å–å¤±è´¥: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 3. ä¸»å¤„ç†å‡½æ•° (ä¿®æ”¹ç‰ˆï¼šåˆ†å—å†™å…¥DTAï¼Œé¿å…å†…å­˜å´©æºƒ)\n",
    "def merge_csv_to_stata_chunks(csv_dir, output_path_base, batch_size=20):\n",
    "    \"\"\"\n",
    "    å°†CSVæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶åˆ†æ‰¹åˆå¹¶ï¼Œå¹¶æœ€ç»ˆç”Ÿæˆå¤šä¸ªåˆ†å—çš„ .dta æ–‡ä»¶ã€‚\n",
    "    è¿™ä¸ªæ–¹æ³•å¯ä»¥å¤„ç†éå¸¸å¤§çš„æ•°æ®é›†ï¼Œå› ä¸ºå®ƒé¿å…äº†ä¸€æ¬¡æ€§å°†æ‰€æœ‰æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­ã€‚\n",
    "    \"\"\"\n",
    "    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç›®å½•æ¥å­˜æ”¾ä¸­é—´çš„ Parquet æ–‡ä»¶\n",
    "    temp_dir = Path(csv_dir) / \"temp_parquet_chunks\"\n",
    "    if temp_dir.exists():\n",
    "        shutil.rmtree(temp_dir) # å¦‚æœæ—§çš„ä¸´æ—¶ç›®å½•å­˜åœ¨ï¼Œå…ˆæ¸…ç©º\n",
    "    temp_dir.mkdir()\n",
    "    \n",
    "    all_files = sorted(glob.glob(os.path.join(csv_dir, \"*.csv\")))\n",
    "    print(f\"ğŸ“‚ å‘ç° {len(all_files)} ä¸ªCSVæ–‡ä»¶ï¼Œå°†ä»¥æ¯æ‰¹ {batch_size} ä¸ªæ–‡ä»¶çš„è§„æ¨¡å¤„ç†ã€‚\")\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"ğŸ¤·â€â™€ï¸ åœ¨æŒ‡å®šç›®å½•æœªæ‰¾åˆ°ä»»ä½•CSVæ–‡ä»¶ï¼Œæ“ä½œç»ˆæ­¢ã€‚\")\n",
    "        return\n",
    "\n",
    "    # --- é˜¶æ®µä¸€ï¼šå°† CSV åˆ†æ‰¹è½¬æ¢ä¸º Parquet ä¸­é—´æ–‡ä»¶ ---\n",
    "    total_batches = (len(all_files) + batch_size - 1) // batch_size\n",
    "    for i in tqdm(range(total_batches), desc=\"å¤„ç†CSVæ‰¹æ¬¡å¹¶ç”ŸæˆParquet\"):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_files = all_files[start_idx:end_idx]\n",
    "        \n",
    "        batch_dfs = [robust_csv_reader(f) for f in batch_files if f]\n",
    "        batch_dfs = [df for df in batch_dfs if df is not None and df.height > 0]\n",
    "\n",
    "        if not batch_dfs:\n",
    "            continue\n",
    "            \n",
    "        # åˆå¹¶å½“å‰æ‰¹æ¬¡çš„ Polars DataFrames\n",
    "        batch_polars_df = pl.concat(batch_dfs)\n",
    "        \n",
    "        # å°†å½“å‰å°æ‰¹æ¬¡ç«‹åˆ»å­˜ä¸º Parquet æ–‡ä»¶ï¼Œå†…å­˜å ç”¨ä½\n",
    "        # Parquet æ˜¯ä¸€ç§é«˜æ•ˆçš„åˆ—å¼å­˜å‚¨æ ¼å¼\n",
    "        parquet_path = temp_dir / f\"batch_{i:04d}.parquet\"\n",
    "        batch_polars_df.write_parquet(parquet_path)\n",
    "\n",
    "    print(\"\\nâœ… æ‰€æœ‰CSVæ‰¹æ¬¡å‡å·²å¤„ç†å¹¶ä¿å­˜ä¸ºä¸´æ—¶çš„ Parquet æ–‡ä»¶ã€‚\")\n",
    "\n",
    "    # --- é˜¶æ®µäºŒï¼šé€ä¸€å°† Parquet ä¸­é—´æ–‡ä»¶è½¬æ¢ä¸º .dta åˆ†å—æ–‡ä»¶ ---\n",
    "    # è¿™æ˜¯é¿å…å†…å­˜å´©æºƒçš„å…³é”®æ­¥éª¤\n",
    "    print(\"â³ å¼€å§‹å°†æ¯ä¸ª Parquet åˆ†å—ç‹¬ç«‹è½¬æ¢ä¸º .dta æ–‡ä»¶...\")\n",
    "    all_parquet_files = sorted(temp_dir.glob(\"*.parquet\"))\n",
    "    \n",
    "    if not all_parquet_files:\n",
    "        print(\"ğŸ¤·â€â™€ï¸ æœªèƒ½ç”Ÿæˆä»»ä½• Parquet ä¸­é—´æ–‡ä»¶ï¼Œæ— æ³•ç»§ç»­ã€‚\")\n",
    "        shutil.rmtree(temp_dir)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        for i, parquet_file in enumerate(tqdm(all_parquet_files, desc=\"è½¬æ¢Parquetåˆ†å—ä¸ºDTA\")):\n",
    "            # a. åªè¯»å–ä¸€ä¸ªåˆ†å—ï¼Œå†…å­˜å ç”¨å°\n",
    "            polars_df_chunk = pl.read_parquet(parquet_file)\n",
    "            \n",
    "            # b. åªå°†è¿™ä¸ªå°åˆ†å—è½¬æ¢ä¸º Pandasï¼Œè¿™æ˜¯æ•´ä¸ªæµç¨‹ä¸­å†…å­˜å ç”¨æœ€é«˜ç‚¹ï¼Œä½†å·²è¢«åˆ†å—é™åˆ¶\n",
    "            pandas_df_chunk = polars_df_chunk.to_pandas()\n",
    "            \n",
    "            # c. å°†è¿™ä¸ªå°åˆ†å—å†™å…¥ä¸€ä¸ªç‹¬ç«‹çš„ .dta æ–‡ä»¶\n",
    "            output_dta_part = f\"{output_path_base}_part_{i:04d}.dta\"\n",
    "            pandas_df_chunk.to_stata(output_dta_part, write_index=False)\n",
    "        \n",
    "        print(f\"\\nâœ… æ‰€æœ‰åˆ†å—å¤„ç†å®Œæ¯•ï¼\")\n",
    "        print(f\"ğŸ“‚ è¯·åœ¨è¾“å‡ºç›®å½•æŸ¥çœ‹ç”Ÿæˆçš„ `..._part_N.dta` æ–‡ä»¶ã€‚\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åœ¨è½¬æ¢ Parquet åˆ° DTA çš„è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "        print(\"ğŸ’¡ è¯·æ£€æŸ¥pandaså’Œç›¸å…³ä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…ã€‚\")\n",
    "    \n",
    "    finally:\n",
    "        # 5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "        print(\"ğŸ—‘ï¸ æ­£åœ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶...\")\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(\"âœ¨ æ“ä½œå®Œæˆã€‚\")\n",
    "\n",
    "\n",
    "# 4. æ‰§è¡Œå…¥å£\n",
    "if __name__ == \"__main__\":\n",
    "    # --- è¯·åœ¨è¿™é‡Œé…ç½®æ‚¨çš„è·¯å¾„ ---\n",
    "    # ä½¿ç”¨ pathlib æ¥æ„å»ºè·¨å¹³å°çš„è·¯å¾„\n",
    "    # root = Path.cwd() # å¦‚æœè„šæœ¬åœ¨é¡¹ç›®æ ¹ç›®å½•çš„å­æ–‡ä»¶å¤¹ä¸­ï¼Œå¯èƒ½éœ€è¦ Path.cwd().parent\n",
    "    root = Path.cwd().parent \n",
    "\n",
    "    # è¾“å…¥æ–‡ä»¶å¤¹ï¼Œå­˜æ”¾æ‚¨æ‰€æœ‰çš„.csvæ–‡ä»¶\n",
    "    input_directory = str(root / 'input')\n",
    "    \n",
    "    # è¾“å‡ºæ–‡ä»¶å¤¹å’Œæ–‡ä»¶åå‰ç¼€\n",
    "    # ä¾‹å¦‚ï¼Œè¿™é‡Œä¼šç”Ÿæˆ merged_data_part_0000.dta, merged_data_part_0001.dta, ...\n",
    "    output_file_base = str(root / 'output' / 'merged_data')\n",
    "\n",
    "    # æ£€æŸ¥å¹¶åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "    Path(root / 'output').mkdir(exist_ok=True)\n",
    "\n",
    "    # --- æ‰§è¡Œä¸»å‡½æ•° ---\n",
    "    merge_csv_to_stata_chunks(\n",
    "        csv_dir=input_directory,\n",
    "        output_path_base=output_file_base,\n",
    "        batch_size=10 #å¯ä»¥æ ¹æ®ä½ çš„å†…å­˜å¤§å°è°ƒæ•´è¿™ä¸ªå€¼ï¼Œè¶Šå°ï¼Œå•æ‰¹æ¬¡å†…å­˜å ç”¨è¶Šä½\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AccRes_New",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
