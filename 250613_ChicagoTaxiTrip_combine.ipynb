{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77140efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/skywings/Documents/RA/Professor_Zhao_RA/任务/chicago-taxi-data')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "root = Path.cwd().parent\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef2e3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 23 columns):\n",
      " #   Column                  Non-Null Count    Dtype  \n",
      "---  ------                  --------------    -----  \n",
      " 0   unique_key              1000000 non-null  object \n",
      " 1   taxi_id                 1000000 non-null  object \n",
      " 2   trip_start_timestamp    1000000 non-null  object \n",
      " 3   trip_end_timestamp      1000000 non-null  object \n",
      " 4   trip_seconds            999628 non-null   float64\n",
      " 5   trip_miles              1000000 non-null  float64\n",
      " 6   pickup_census_tract     23825 non-null    float64\n",
      " 7   dropoff_census_tract    35542 non-null    float64\n",
      " 8   pickup_community_area   30401 non-null    float64\n",
      " 9   dropoff_community_area  73067 non-null    float64\n",
      " 10  fare                    999904 non-null   float64\n",
      " 11  tips                    999904 non-null   float64\n",
      " 12  tolls                   996680 non-null   float64\n",
      " 13  extras                  999904 non-null   float64\n",
      " 14  trip_total              999904 non-null   float64\n",
      " 15  payment_type            1000000 non-null  object \n",
      " 16  company                 3224 non-null     object \n",
      " 17  pickup_latitude         30405 non-null    float64\n",
      " 18  pickup_longitude        30405 non-null    float64\n",
      " 19  pickup_location         30405 non-null    object \n",
      " 20  dropoff_latitude        73255 non-null    float64\n",
      " 21  dropoff_longitude       73255 non-null    float64\n",
      " 22  dropoff_location        73255 non-null    object \n",
      "dtypes: float64(15), object(8)\n",
      "memory usage: 175.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_key</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>trip_end_timestamp</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pickup_census_tract</th>\n",
       "      <th>dropoff_census_tract</th>\n",
       "      <th>pickup_community_area</th>\n",
       "      <th>dropoff_community_area</th>\n",
       "      <th>...</th>\n",
       "      <th>extras</th>\n",
       "      <th>trip_total</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>company</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_location</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18b092309dff1fa8298b21f8dfb9be19124363c4</td>\n",
       "      <td>193f49f2e6f3c31a0058bb26e322f818043e6d64ff80a9...</td>\n",
       "      <td>2014-06-07 23:15:00 UTC</td>\n",
       "      <td>2014-06-07 23:15:00 UTC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.703182e+10</td>\n",
       "      <td>1.703182e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b0601c2eaeccb6c7629f48858a007afea2e98e4</td>\n",
       "      <td>2cb7deb7674470467d31b1bba4657ab1c44c1feebf6274...</td>\n",
       "      <td>2014-04-16 15:30:00 UTC</td>\n",
       "      <td>2014-04-16 15:30:00 UTC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.986712</td>\n",
       "      <td>-87.663416</td>\n",
       "      <td>POINT (-87.6634164054 41.9867117999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0e1861cfb146b9e1b8b773b5be89f69cee92a0fc</td>\n",
       "      <td>e055c27835840cb1b08c8f68e20066307ab235a02fe7bd...</td>\n",
       "      <td>2014-04-25 14:15:00 UTC</td>\n",
       "      <td>2014-04-25 14:15:00 UTC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f8583c9d5d9fde21f8bcdd7b3ed0439c21a7fd88</td>\n",
       "      <td>e055c27835840cb1b08c8f68e20066307ab235a02fe7bd...</td>\n",
       "      <td>2014-04-25 14:15:00 UTC</td>\n",
       "      <td>2014-04-25 14:15:00 UTC</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5551bd37929928c32a1e703f871cedd1978709f0</td>\n",
       "      <td>e055c27835840cb1b08c8f68e20066307ab235a02fe7bd...</td>\n",
       "      <td>2014-04-25 16:30:00 UTC</td>\n",
       "      <td>2014-04-25 16:30:00 UTC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 unique_key  \\\n",
       "0  18b092309dff1fa8298b21f8dfb9be19124363c4   \n",
       "1  7b0601c2eaeccb6c7629f48858a007afea2e98e4   \n",
       "2  0e1861cfb146b9e1b8b773b5be89f69cee92a0fc   \n",
       "3  f8583c9d5d9fde21f8bcdd7b3ed0439c21a7fd88   \n",
       "4  5551bd37929928c32a1e703f871cedd1978709f0   \n",
       "\n",
       "                                             taxi_id     trip_start_timestamp  \\\n",
       "0  193f49f2e6f3c31a0058bb26e322f818043e6d64ff80a9...  2014-06-07 23:15:00 UTC   \n",
       "1  2cb7deb7674470467d31b1bba4657ab1c44c1feebf6274...  2014-04-16 15:30:00 UTC   \n",
       "2  e055c27835840cb1b08c8f68e20066307ab235a02fe7bd...  2014-04-25 14:15:00 UTC   \n",
       "3  e055c27835840cb1b08c8f68e20066307ab235a02fe7bd...  2014-04-25 14:15:00 UTC   \n",
       "4  e055c27835840cb1b08c8f68e20066307ab235a02fe7bd...  2014-04-25 16:30:00 UTC   \n",
       "\n",
       "        trip_end_timestamp  trip_seconds  trip_miles  pickup_census_tract  \\\n",
       "0  2014-06-07 23:15:00 UTC           0.0         0.0         1.703182e+10   \n",
       "1  2014-04-16 15:30:00 UTC           0.0         0.0                  NaN   \n",
       "2  2014-04-25 14:15:00 UTC           0.0         0.0                  NaN   \n",
       "3  2014-04-25 14:15:00 UTC          60.0         0.0                  NaN   \n",
       "4  2014-04-25 16:30:00 UTC           0.0         0.0                  NaN   \n",
       "\n",
       "   dropoff_census_tract  pickup_community_area  dropoff_community_area  ...  \\\n",
       "0          1.703182e+10                    NaN                     NaN  ...   \n",
       "1                   NaN                    NaN                    77.0  ...   \n",
       "2                   NaN                    NaN                     NaN  ...   \n",
       "3                   NaN                    NaN                     NaN  ...   \n",
       "4                   NaN                    NaN                     NaN  ...   \n",
       "\n",
       "   extras  trip_total  payment_type  company  pickup_latitude  \\\n",
       "0     NaN         NaN          Cash      NaN              NaN   \n",
       "1     NaN         NaN   Credit Card      NaN              NaN   \n",
       "2     NaN         NaN   Credit Card      NaN              NaN   \n",
       "3     NaN         NaN   Credit Card      NaN              NaN   \n",
       "4     NaN         NaN   Credit Card      NaN              NaN   \n",
       "\n",
       "  pickup_longitude pickup_location  dropoff_latitude  dropoff_longitude  \\\n",
       "0              NaN             NaN               NaN                NaN   \n",
       "1              NaN             NaN         41.986712         -87.663416   \n",
       "2              NaN             NaN               NaN                NaN   \n",
       "3              NaN             NaN               NaN                NaN   \n",
       "4              NaN             NaN               NaN                NaN   \n",
       "\n",
       "                       dropoff_location  \n",
       "0                                   NaN  \n",
       "1  POINT (-87.6634164054 41.9867117999)  \n",
       "2                                   NaN  \n",
       "3                                   NaN  \n",
       "4                                   NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sample = pd.read_csv(root / 'input' / 'chicago_taxi_trips_000000000003.csv')\n",
    "display(df_sample.head(), df_sample.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c5a735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 发现 212 个CSV文件，将以每批 10 个文件的规模处理。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理CSV批次并生成Parquet:   0%|          | 0/22 [00:00<?, ?it/s]/var/folders/gx/8b80zx6d2dd0slh0770_q04w0000gn/T/ipykernel_33918/1042761720.py:34: DeprecationWarning: the argument `dtypes` for `read_csv` is deprecated. It was renamed to `schema_overrides` in version 0.20.31.\n",
      "  df = pl.read_csv(\n",
      "处理CSV批次并生成Parquet: 100%|██████████| 22/22 [04:01<00:00, 10.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 所有CSV批次均已处理并保存为临时的 Parquet 文件。\n",
      "⏳ 开始将每个 Parquet 分块独立转换为 .dta 文件...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "转换Parquet分块为DTA: 100%|██████████| 22/22 [16:02<00:00, 43.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 所有分块处理完毕！\n",
      "📂 请在输出目录查看生成的 `..._part_N.dta` 文件。\n",
      "🗑️ 正在清理临时文件...\n",
      "✨ 操作完成。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil  # 用于处理文件夹\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. 标准结构定义 (与您原来的一致，无需改动)\n",
    "# 定义所有列的统一数据类型和顺序\n",
    "STANDARD_SCHEMA = {\n",
    "    \"unique_key\": pl.Utf8, \"taxi_id\": pl.Utf8, \"trip_start_timestamp\": pl.Utf8,\n",
    "    \"trip_end_timestamp\": pl.Utf8, \"trip_seconds\": pl.Float64, \"trip_miles\": pl.Float64,\n",
    "    \"pickup_census_tract\": pl.Float64, \"dropoff_census_tract\": pl.Float64,\n",
    "    \"pickup_community_area\": pl.Float64, \"dropoff_community_area\": pl.Float64,\n",
    "    \"fare\": pl.Float64, \"tips\": pl.Float64, \"tolls\": pl.Float64,\n",
    "    \"extras\": pl.Float64, \"trip_total\": pl.Float64, \"payment_type\": pl.Utf8,\n",
    "    \"company\": pl.Utf8, \"pickup_latitude\": pl.Float64, \"pickup_longitude\": pl.Float64,\n",
    "    \"pickup_location\": pl.Utf8, \"dropoff_latitude\": pl.Float64,\n",
    "    \"dropoff_longitude\": pl.Float64, \"dropoff_location\": pl.Utf8\n",
    "}\n",
    "STANDARD_COLS = list(STANDARD_SCHEMA.keys())\n",
    "\n",
    "# 2. 安全读取CSV的函数 (与您原来的一致，无需改动)\n",
    "# 这个函数能健壮地处理列名不一致、列缺失等问题\n",
    "def robust_csv_reader(file_path):\n",
    "    \"\"\"\n",
    "    安全地读取单个CSV文件，处理可能的错误，并将其转换为标准化的Polars DataFrame。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 使用 Polars 读取 CSV，初始全部作为字符串类型，避免推断错误\n",
    "        df = pl.read_csv(\n",
    "            file_path, infer_schema_length=0,\n",
    "            dtypes={col: pl.Utf8 for col in STANDARD_COLS},\n",
    "            null_values=[\"\", \"NA\", \"NULL\", \"N/A\", \"NaN\", \".\"],\n",
    "            ignore_errors=True\n",
    "        )\n",
    "        # 清理列名：转为小写，替换空格为下划线\n",
    "        df = df.rename(lambda col: col.strip().lower().replace(\" \", \"_\"))\n",
    "        # 确保所有标准列都存在，不存在则用null填充\n",
    "        for col in STANDARD_COLS:\n",
    "            if col not in df.columns:\n",
    "                df = df.with_columns(pl.lit(None, dtype=pl.Utf8).alias(col))\n",
    "        # 选择标准列并按照标准schema进行类型转换\n",
    "        return df.select(STANDARD_COLS).cast(STANDARD_SCHEMA, strict=False)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 文件 {os.path.basename(file_path)} 读取失败: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 3. 主处理函数 (修改版：分块写入DTA，避免内存崩溃)\n",
    "def merge_csv_to_stata_chunks(csv_dir, output_path_base, batch_size=20):\n",
    "    \"\"\"\n",
    "    将CSV文件夹中的文件分批合并，并最终生成多个分块的 .dta 文件。\n",
    "    这个方法可以处理非常大的数据集，因为它避免了一次性将所有数据加载到内存中。\n",
    "    \"\"\"\n",
    "    # 创建一个临时目录来存放中间的 Parquet 文件\n",
    "    temp_dir = Path(csv_dir) / \"temp_parquet_chunks\"\n",
    "    if temp_dir.exists():\n",
    "        shutil.rmtree(temp_dir) # 如果旧的临时目录存在，先清空\n",
    "    temp_dir.mkdir()\n",
    "    \n",
    "    all_files = sorted(glob.glob(os.path.join(csv_dir, \"*.csv\")))\n",
    "    print(f\"📂 发现 {len(all_files)} 个CSV文件，将以每批 {batch_size} 个文件的规模处理。\")\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"🤷‍♀️ 在指定目录未找到任何CSV文件，操作终止。\")\n",
    "        return\n",
    "\n",
    "    # --- 阶段一：将 CSV 分批转换为 Parquet 中间文件 ---\n",
    "    total_batches = (len(all_files) + batch_size - 1) // batch_size\n",
    "    for i in tqdm(range(total_batches), desc=\"处理CSV批次并生成Parquet\"):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_files = all_files[start_idx:end_idx]\n",
    "        \n",
    "        batch_dfs = [robust_csv_reader(f) for f in batch_files if f]\n",
    "        batch_dfs = [df for df in batch_dfs if df is not None and df.height > 0]\n",
    "\n",
    "        if not batch_dfs:\n",
    "            continue\n",
    "            \n",
    "        # 合并当前批次的 Polars DataFrames\n",
    "        batch_polars_df = pl.concat(batch_dfs)\n",
    "        \n",
    "        # 将当前小批次立刻存为 Parquet 文件，内存占用低\n",
    "        # Parquet 是一种高效的列式存储格式\n",
    "        parquet_path = temp_dir / f\"batch_{i:04d}.parquet\"\n",
    "        batch_polars_df.write_parquet(parquet_path)\n",
    "\n",
    "    print(\"\\n✅ 所有CSV批次均已处理并保存为临时的 Parquet 文件。\")\n",
    "\n",
    "    # --- 阶段二：逐一将 Parquet 中间文件转换为 .dta 分块文件 ---\n",
    "    # 这是避免内存崩溃的关键步骤\n",
    "    print(\"⏳ 开始将每个 Parquet 分块独立转换为 .dta 文件...\")\n",
    "    all_parquet_files = sorted(temp_dir.glob(\"*.parquet\"))\n",
    "    \n",
    "    if not all_parquet_files:\n",
    "        print(\"🤷‍♀️ 未能生成任何 Parquet 中间文件，无法继续。\")\n",
    "        shutil.rmtree(temp_dir)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        for i, parquet_file in enumerate(tqdm(all_parquet_files, desc=\"转换Parquet分块为DTA\")):\n",
    "            # a. 只读取一个分块，内存占用小\n",
    "            polars_df_chunk = pl.read_parquet(parquet_file)\n",
    "            \n",
    "            # b. 只将这个小分块转换为 Pandas，这是整个流程中内存占用最高点，但已被分块限制\n",
    "            pandas_df_chunk = polars_df_chunk.to_pandas()\n",
    "            \n",
    "            # c. 将这个小分块写入一个独立的 .dta 文件\n",
    "            output_dta_part = f\"{output_path_base}_part_{i:04d}.dta\"\n",
    "            pandas_df_chunk.to_stata(output_dta_part, write_index=False)\n",
    "        \n",
    "        print(f\"\\n✅ 所有分块处理完毕！\")\n",
    "        print(f\"📂 请在输出目录查看生成的 `..._part_N.dta` 文件。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 在转换 Parquet 到 DTA 的过程中发生错误: {e}\")\n",
    "        print(\"💡 请检查pandas和相关依赖是否正确安装。\")\n",
    "    \n",
    "    finally:\n",
    "        # 5. 清理临时文件\n",
    "        print(\"🗑️ 正在清理临时文件...\")\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(\"✨ 操作完成。\")\n",
    "\n",
    "\n",
    "# 4. 执行入口\n",
    "if __name__ == \"__main__\":\n",
    "    # --- 请在这里配置您的路径 ---\n",
    "    # 使用 pathlib 来构建跨平台的路径\n",
    "    # root = Path.cwd() # 如果脚本在项目根目录的子文件夹中，可能需要 Path.cwd().parent\n",
    "    root = Path.cwd().parent \n",
    "\n",
    "    # 输入文件夹，存放您所有的.csv文件\n",
    "    input_directory = str(root / 'input')\n",
    "    \n",
    "    # 输出文件夹和文件名前缀\n",
    "    # 例如，这里会生成 merged_data_part_0000.dta, merged_data_part_0001.dta, ...\n",
    "    output_file_base = str(root / 'output' / 'merged_data')\n",
    "\n",
    "    # 检查并创建输出目录\n",
    "    Path(root / 'output').mkdir(exist_ok=True)\n",
    "\n",
    "    # --- 执行主函数 ---\n",
    "    merge_csv_to_stata_chunks(\n",
    "        csv_dir=input_directory,\n",
    "        output_path_base=output_file_base,\n",
    "        batch_size=10 #可以根据你的内存大小调整这个值，越小，单批次内存占用越低\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AccRes_New",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
